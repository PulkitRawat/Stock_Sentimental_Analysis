{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2226acf-472e-41c9-9bd9-d60ee1677c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f86babb-e150-4ef2-85b7-c686b8f1858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3cafb586-25a2-47f2-bfe1-b3e0f0f4b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf9130-c962-42f9-8e0a-94093373f097",
   "metadata": {},
   "source": [
    "# Scraping news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69b51c4c-f922-44e5-b287-1769b0bfbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(query, start_date, end_date, max_entries = 5):\n",
    "    \"\"\"\n",
    "    Scrapes news articles from Google News RSS for a given query and filters them by data.\n",
    "\n",
    "    Parameters:\n",
    "        query(str): the stock or company name to search for.\n",
    "        start_date(date): the start date for filtering articles.\n",
    "        end_date(date): the end date for filtering articles.\n",
    "        max_entries(int): number of times to retry incase of request failure.\n",
    "\n",
    "    returns:\n",
    "        pd.DataFrame: a dataframe consisting of article title, link, and publication date\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    url = f\"https://news.google.com/rss/search?q={query}+after:{start_date}+before:{end_date}\"\n",
    "    for i in range(max_entries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            break\n",
    "        except exception as e:\n",
    "            print(f\"an error occured in requesting {e}\")\n",
    "            print(f\"retrying attempt {i}\")\n",
    "            time.sleep(random.uniform(2,10))\n",
    "        \n",
    "    soup = BeautifulSoup(response.content, 'xml')\n",
    "    items = soup.find_all('item')\n",
    "    \n",
    "    for item in items:\n",
    "        title = item.title.text\n",
    "        link = item.link.text\n",
    "        pub_date = item.pubDate.text\n",
    "        articles.append({'title': title, 'link': link, 'pub_date': pub_date})\n",
    "    return pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5595a096-9581-4684-9bb0-b09c795591d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_over_date_range(query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles over a given date range by iterating through months.\n",
    "\n",
    "    Parameters:\n",
    "        query(str): The stock or company name to search for.\n",
    "        start_date(str): the start date in 'yyyy-mm-dd' format.\n",
    "        end_date(str): the end date in 'yyyy-mm-dd' format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame collected across the date range.\n",
    "    \"\"\"\n",
    "    start_date_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    end_date_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "\n",
    "    all_articles = pd.DataFrame()\n",
    "\n",
    "    current_date = start_date_dt\n",
    "\n",
    "    while current_date<end_date_dt:\n",
    "        next_date = min(current_date + relativedelta(months = 1), end_date_dt)\n",
    "        \n",
    "        start_str = current_date.strftime('%Y-%m-%d')\n",
    "        end_str = next_date.strftime('%Y-%m-%d')\n",
    "        month_articles = scrape_news(query, start_str, end_str)\n",
    "        all_articles = pd.concat([all_articles, month_articles], ignore_index=True)\n",
    "        current_date = next_date\n",
    "\n",
    "    return all_articles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35a0e72d-41a4-40e1-bd77-0da0a24407cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_news(queries, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles for mutliple queries over a gives date range and saves each result to a csv file\n",
    "\n",
    "    Parameters:\n",
    "        queries(list): A list of stock or company names to search for.\n",
    "        start_date(str): the start date in 'yyyy-mm-dd' format.\n",
    "        end_date(str): the end date in 'yyyy-mm-dd' format.\n",
    "\n",
    "    Outputs:\n",
    "        CSV files named after each query, containing scraped news articles.\n",
    "    \"\"\"\n",
    "    for query in queries:\n",
    "        articles = scrape_news_over_date_range(query, start_date, end_date)\n",
    "        output_csv = f\"{query}.csv\"\n",
    "        articles.to_csv(output_csv, index=False)\n",
    "        print(f\"Saved {query} articles to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb3dbfee-fc46-438b-9224-6f80474d353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Reliance articles to Reliance.csv\n",
      "Saved Microsoft articles to Microsoft.csv\n",
      "Saved Google articles to Google.csv\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Reliance\", \"Microsoft\", \"Google\"]\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2022-12-31'\n",
    "scrape_and_save_news(queries, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f7c8244-ba3c-4bcb-9db2-906b13645914",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df1 = pd.read_csv('Reliance.csv')\n",
    "news_df2 = pd.read_csv('Microsoft.csv')\n",
    "news_df3 = pd.read_csv('Google.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8daea83-2899-449f-a816-4ef36336d2b8",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2f62dd0-6c73-4d10-9824-21c27a03d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    Cleans the text and returns in form of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    text(str): string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    str: a joint of tokens created.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    tokens = [token.lemma_ for token in doc \n",
    "             if not token.is_stop \n",
    "             and not token.is_punct \n",
    "             and token.text not in [\"'\", '\"']]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "058e4575-f7b7-47e6-9ed9-3c8ce1f122ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df1['cleaned_text'] = news_df1['title'].apply(clean_text)\n",
    "news_df2['cleaned_text'] = news_df2['title'].apply(clean_text)\n",
    "news_df2['cleaned_text'] = news_df2['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd2e92-0eb7-4710-b319-e8aa9e8a7430",
   "metadata": {},
   "source": [
    "# Extracting Stock Prices for supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "394bb85a-df17-42c3-9adf-16dbb9509229",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2022-01-01'\n",
    "end_date = '2022-12-31'\n",
    "\n",
    "stock_symbols = {'Reliance': 'RELIANCE.NS', 'Microcosft': 'MSFT', 'Google': 'GOOGL'}\n",
    "\n",
    "date_range = pd.date_range(start = start_date, end = end_date, freq = 'B') # 'B' for business days\n",
    "\n",
    "def fetch_stock_data(symbol:str, max_entries =3):\n",
    "    \"\"\"\n",
    "    fetches stock data from yahoo.\n",
    "\n",
    "    Parameters:\n",
    "    symbol(str): symbol corresponding to the country name.\n",
    "    max_entries(int): no. of retries in fetching if the fetch fails.\n",
    "\n",
    "    Returns: \n",
    "    pd.DataFrame: dataset containing stockprices\n",
    "    \"\"\"\n",
    "    for i in range(max_entries):\n",
    "        try:\n",
    "            stock_data = yf.download(symbol, start= start_date, end= end_date)\n",
    "        except Exception as e:\n",
    "            print (f'error fetching data {e}; retrying attempt {i}.....')\n",
    "        time.sleep(random.uniform(2,5))\n",
    "\n",
    "        return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aa048afd-cda9-4809-b4b3-c0123f4a2707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_price for Reliance saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_price for Microcosft saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_price for Google saved\n"
     ]
    }
   ],
   "source": [
    "for company, symbol in stock_symbols.items():\n",
    "    stock_data = fetch_stock_data(symbol)\n",
    "    stock_data.reset_index(inplace = True)\n",
    "    output_csv = f'{company}_stock_prices.csv'\n",
    "    stock_data.to_csv(output_csv, index = False)\n",
    "    print(f\"stock_price for {company} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d5ffdc25-8958-42c1-b486-00bf831a8a0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Microsoft_stock_prices.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stock_df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReliance_stock_prices.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m stock_df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMicrosoft_stock_prices.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m stock_df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGoogle_stock_prices.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Microsoft_stock_prices.csv'"
     ]
    }
   ],
   "source": [
    "stock_df1 = pd.read_csv('Reliance_stock_prices.csv')\n",
    "stock_df2 = pd.read_csv('Microsoft_stock_prices.csv')\n",
    "stock_df2 = pd.read_csv('Google_stock_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52493ee1-be40-4118-a04d-8ee956bd0ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2226acf-472e-41c9-9bd9-d60ee1677c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f86babb-e150-4ef2-85b7-c686b8f1858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf9130-c962-42f9-8e0a-94093373f097",
   "metadata": {},
   "source": [
    "# Scraping news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69b51c4c-f922-44e5-b287-1769b0bfbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles from Google News RSS for a given query and filters them by data.\n",
    "\n",
    "    Parameters:\n",
    "        query(str): the stock or company name to search for.\n",
    "        start_date(date): the start date for filtering articles.\n",
    "        end_date(date): the end date for filtering articles.\n",
    "        max_entries(int): number of times to retry incase of request failure.\n",
    "\n",
    "    returns:\n",
    "        pd.DataFrame: a dataframe consisting of article title, link, and publication date\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    url = f\"https://news.google.com/rss/search?q={query}+after:{start_date}+before:{end_date}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except exception as e:\n",
    "        print(f\"an error occured in requesting {e}\")\n",
    "        return\n",
    "    soup = BeautifulSoup(response.content, 'xml')\n",
    "    items = soup.find_all('item')\n",
    "    \n",
    "    for item in items:\n",
    "        title = item.title.text\n",
    "        link = item.link.text\n",
    "        pub_date = item.pubDate.text\n",
    "        articles.append({'title': title, 'link': link, 'pub_date': pub_date})\n",
    "    return pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5595a096-9581-4684-9bb0-b09c795591d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_over_date_range(query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles over a given date range by iterating through months.\n",
    "\n",
    "    Parameters:\n",
    "        query(str): The stock or company name to search for.\n",
    "        start_date(str): the start date in 'yyyy-mm-dd' format.\n",
    "        end_date(str): the end date in 'yyyy-mm-dd' format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame collected across the date range.\n",
    "    \"\"\"\n",
    "    start_date_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    end_date_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "\n",
    "    all_articles = pd.DataFrame()\n",
    "\n",
    "    current_date = start_date_dt\n",
    "\n",
    "    while current_date<end_date_dt:\n",
    "        next_date = min(current_date + relativedelta(months = 1), end_date_dt)\n",
    "        \n",
    "        start_str = current_date.strftime('%Y-%m-%d')\n",
    "        end_str = next_date.strftime('%Y-%m-%d')\n",
    "        month_articles = scrape_news(query, start_str, end_str)\n",
    "        all_articles = pd.concat([all_articles, month_articles], ignore_index=True)\n",
    "        current_date = next_date\n",
    "\n",
    "    return all_articles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35a0e72d-41a4-40e1-bd77-0da0a24407cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_news(queries, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles for mutliple queries over a gives date range and saves each result to a csv file\n",
    "\n",
    "    Parameters:\n",
    "        queries(list): A list of stock or company names to search for.\n",
    "        start_date(str): the start date in 'yyyy-mm-dd' format.\n",
    "        end_date(str): the end date in 'yyyy-mm-dd' format.\n",
    "\n",
    "    Outputs:\n",
    "        CSV files named after each query, containing scraped news articles.\n",
    "    \"\"\"\n",
    "    for query in queries:\n",
    "        articles = scrape_news_over_date_range(query, start_date, end_date)\n",
    "        output_csv = f\"{query}.csv\"\n",
    "        articles.to_csv(output_csv, index=False)\n",
    "        print(f\"Saved {query} articles to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb3dbfee-fc46-438b-9224-6f80474d353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Reliance articles to Reliance.csv\n",
      "Saved Microsoft articles to Microsoft.csv\n",
      "Saved Google articles to Google.csv\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Reliance\", \"Microsoft\", \"Google\"]\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2022-12-31'\n",
    "scrape_and_save_news(queries, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f7c8244-ba3c-4bcb-9db2-906b13645914",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df1 = pd.read_csv('Reliance.csv')\n",
    "news_df2 = pd.read_csv('Microsoft.csv')\n",
    "news_df3 = pd.read_csv('Google.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8daea83-2899-449f-a816-4ef36336d2b8",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2f62dd0-6c73-4d10-9824-21c27a03d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    Cleans the text and returns in form of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    text(str): string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    str: a joint of tokens created.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https\\S+', '', text);\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text);\n",
    "    # text = re.sub(r'\\s+', '', text);\n",
    "   \n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "058e4575-f7b7-47e6-9ed9-3c8ce1f122ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df1['cleaned_text'] = news_df1['title'].apply(clean_text)\n",
    "news_df2['cleaned_text'] = news_df2['title'].apply(clean_text)\n",
    "news_df2['cleaned_text'] = news_df2['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9110030e-76d2-4635-af03-309b5c80f98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reliance jio transformed indias telecom industry five charts mint',\n",
       " 'reliance industries posts record q3 profit rs 11640 crore revenue 14 business today',\n",
       " 'reliance launches new road project counter pushback plastics business standard',\n",
       " 'ril lays road plastic waste hindu',\n",
       " 'reliance enter restaurant business armani etretail']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df1['cleaned_text'][:5].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312cdce-e640-4e74-a729-f94a46ea3e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

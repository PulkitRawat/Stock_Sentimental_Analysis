{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2226acf-472e-41c9-9bd9-d60ee1677c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f86babb-e150-4ef2-85b7-c686b8f1858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69b51c4c-f922-44e5-b287-1769b0bfbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles from Google News RSS for a given query and filters them by data.\n",
    "\n",
    "    Parameters:\n",
    "        query(str): the stock or company name to search for.\n",
    "        start_date(date): the start date for filtering articles.\n",
    "        end_date(date): the end date for filtering articles.\n",
    "        max_entries(int): number of times to retry incase of request failure.\n",
    "\n",
    "    returns:\n",
    "        pd.DataFrame: a dataframe consisting of article title, link, and publication date\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    url = f\"https://news.google.com/rss/search?q={query}+after:{start_date}+before:{end_date}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'xml')\n",
    "    items = soup.find_all('item')\n",
    "\n",
    "    for item in items:\n",
    "        title = item.title.text\n",
    "        link = item.link.text\n",
    "        pub_date = item.pubDate.text\n",
    "        articles.append({'title': title, 'link': link, 'pub_date': pub_date})\n",
    "    return pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5595a096-9581-4684-9bb0-b09c795591d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_over_date_range(query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles over a given date range by iterating through months.\n",
    "\n",
    "    Parameters:\n",
    "        query(str): The stock or company name to search for.\n",
    "        start_date(str): the start date in 'yyyy-mm-dd' format.\n",
    "        end_date(str): the end date in 'yyyy-mm-dd' format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame collected across the date range.\n",
    "    \"\"\"\n",
    "    start_date_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    end_date_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "\n",
    "    all_articles = pd.DataFrame()\n",
    "\n",
    "    current_date = start_date_dt\n",
    "\n",
    "    while current_date<end_date_dt:\n",
    "        next_date = min(current_date + relativedelta(months = 1), end_date_dt)\n",
    "        \n",
    "        start_str = current_date.strftime('%Y-%m-%d')\n",
    "        end_str = next_date.strftime('%Y-%m-%d')\n",
    "        month_articles = scrape_news(query, start_str, end_str)\n",
    "        all_articles = pd.concat([all_articles, month_articles], ignore_index=True)\n",
    "        current_date = next_date\n",
    "\n",
    "    return all_articles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35a0e72d-41a4-40e1-bd77-0da0a24407cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_news(queries, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes news articles for mutliple queries over a gives date range and saves each result to a csv file\n",
    "\n",
    "    Parameters:\n",
    "        queries(list): A list of stock or company names to search for.\n",
    "        start_date(str): the start date in 'yyyy-mm-dd' format.\n",
    "        end_date(str): the end date in 'yyyy-mm-dd' format.\n",
    "\n",
    "    Outputs:\n",
    "        CSV files named after each query, containing scraped news articles.\n",
    "    \"\"\"\n",
    "    for query in queries:\n",
    "        articles = scrape_news_over_date_range(query, start_date, end_date)\n",
    "        output_csv = f\"{query}.csv\"\n",
    "        articles.to_csv(output_csv, index=False)\n",
    "        print(f\"Saved {query} articles to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb3dbfee-fc46-438b-9224-6f80474d353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Reliance articles to Reliance.csv\n",
      "Saved Microsoft articles to Microsoft.csv\n",
      "Saved Google articles to Google.csv\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Reliance\", \"Microsoft\", \"Google\"]\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2022-12-31'\n",
    "scrape_and_save_news(queries, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c8244-ba3c-4bcb-9db2-906b13645914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
